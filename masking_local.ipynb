{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lottery Notebook\n",
    "\n",
    "### imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "import matplotlib.pyplot as plt\n",
    "from pruning import Pruning_tool\n",
    "import copy\n",
    "import time\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('using', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc2 = nn.Linear(400, 200)\n",
    "        self.fc3 = nn.Linear(200,100)\n",
    "        self.fc41 = nn.Linear(100, 32)\n",
    "        self.fc42 = nn.Linear(100, 32)\n",
    "        self.fc5 = nn.Linear(32, 100)\n",
    "        self.fc6 = nn.Linear(100,200)\n",
    "        self.fc7 = nn.Linear(200,400)\n",
    "        self.fc8 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc3(self.fc2(self.fc1(x))))\n",
    "        return self.fc41(h1), self.fc42(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc7(self.fc6(self.fc5(z))))\n",
    "        return torch.sigmoid(self.fc8(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of VAE(\n",
      "  (fc1): Linear(in_features=784, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fc41): Linear(in_features=100, out_features=32, bias=True)\n",
      "  (fc42): Linear(in_features=100, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=100, bias=True)\n",
      "  (fc6): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (fc7): Linear(in_features=200, out_features=400, bias=True)\n",
      "  (fc8): Linear(in_features=400, out_features=784, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "log_interval = 100\n",
    "\n",
    "mnist_trainset = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "                   batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mnist_testset = test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "                  batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = VAE().to(device)\n",
    "print(model.parameters)\n",
    "optimizer = optim.Adam(model.parameters(), lr=7e-4)\n",
    "rewind_state_dict = copy.deepcopy(model.state_dict()) \n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_anneal_function(anneal_function, step, k, x0):\n",
    "    \"\"\" Beta update function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        anneal_function : string\n",
    "            What type of update (logisitc or linear)\n",
    "        step : int\n",
    "            Which step of the training\n",
    "        k : float\n",
    "            Coefficient of the logistic function\n",
    "        x0 : float\n",
    "            Delay of the logistic function or slope of the linear function\n",
    "        Returns\n",
    "        -------\n",
    "        beta : float\n",
    "            Weight of the KL divergence in the loss function \n",
    "        \"\"\"\n",
    "    if anneal_function == 'logistic':\n",
    "        return float(1/(1+np.exp(-k*(step-x0))))\n",
    "    elif anneal_function == 'linear':\n",
    "        return min(1, step/x0)\n",
    "\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, beta):\n",
    "    \"\"\" Compute the loss function between recon_x (output of the VAE) \n",
    "    and x (input of the VAE)\n",
    "    \"\"\"\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = 0.5 * torch.sum(logvar.exp() + mu.pow(2) - 1 - logvar)\n",
    "    return BCE + beta*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 444.865509\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 200.020233\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 212.490952\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 188.451279\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 184.191925\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 160.402390\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 169.397400\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 155.762924\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 156.988495\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 155.283737\n",
      "====> Epoch: 0 Average loss: 180.5791\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 145.979568\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 141.154205\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 133.171875\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 129.308990\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 136.864258\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 133.485840\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 136.080261\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 123.717049\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 133.861984\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 129.512360\n",
      "====> Epoch: 1 Average loss: 134.5248\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 122.665245\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 126.990326\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 115.628784\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 121.605591\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 118.578384\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 122.842812\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 120.648224\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 119.716599\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 116.219292\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 118.828430\n",
      "====> Epoch: 2 Average loss: 121.3537\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 119.681168\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 110.763824\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 126.002747\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 113.852226\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 122.607132\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 105.581482\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 112.444504\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 117.158134\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 117.812241\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 112.885406\n",
      "====> Epoch: 3 Average loss: 116.1898\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 117.396255\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 123.154648\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 110.686478\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 118.448326\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 119.968857\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 121.305054\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 114.600014\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 110.642349\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 118.852097\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 113.438614\n",
      "====> Epoch: 4 Average loss: 114.4453\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 109.778946\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 118.712875\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 114.491516\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 113.609528\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 107.812317\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 115.486649\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 112.091576\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 109.510117\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 108.675507\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 118.573875\n",
      "====> Epoch: 5 Average loss: 113.4486\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 107.241219\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 113.416199\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 109.480652\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 111.048004\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 114.998390\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 111.451424\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 111.356308\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 109.789925\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 115.968796\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 110.514618\n",
      "====> Epoch: 6 Average loss: 112.6476\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 109.175026\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 106.795860\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 109.817116\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 114.086082\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 111.140808\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 110.515251\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 117.537514\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 111.277390\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 114.119827\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 105.865051\n",
      "====> Epoch: 7 Average loss: 112.1565\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 111.005661\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 110.459602\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 112.572670\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 112.884956\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 115.489265\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 114.405655\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 108.758636\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 109.533966\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 113.498932\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 109.772385\n",
      "====> Epoch: 8 Average loss: 111.8000\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 109.011475\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 107.757080\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 115.121384\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 112.665665\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 110.381500\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 113.545128\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 110.062515\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 114.133896\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 109.575714\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 107.901886\n",
      "====> Epoch: 9 Average loss: 111.4733\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 119.678940\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 107.983398\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 116.891449\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 118.240112\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 107.206314\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 117.284744\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 114.533813\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 116.073746\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 115.929886\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 111.826805\n",
      "====> Epoch: 10 Average loss: 111.2019\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 106.674545\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 103.010857\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 109.987137\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 113.043335\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 106.808807\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 107.397644\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 114.004440\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 110.721863\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 107.312775\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 114.182358\n",
      "====> Epoch: 11 Average loss: 111.0004\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 109.465828\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 111.591415\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 114.061546\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 105.344116\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 112.555862\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 110.392426\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 109.289139\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 111.156647\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 113.383484\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 109.151947\n",
      "====> Epoch: 12 Average loss: 110.8052\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 108.002380\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 112.834961\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 109.126984\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 108.523094\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 107.722809\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 109.703873\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 115.876556\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 116.593483\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 115.167732\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 107.413147\n",
      "====> Epoch: 13 Average loss: 110.6306\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 113.305016\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 110.932961\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 110.250435\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 113.476700\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 115.032417\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 105.337677\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 106.091393\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 109.002129\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 112.467216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 108.944412\n",
      "====> Epoch: 14 Average loss: 110.5062\n"
     ]
    }
   ],
   "source": [
    "plt_loss = []\n",
    "for name, param in model.named_parameters(): \n",
    "    param.data = rewind_state_dict[name].clone()\n",
    "for epoch in range(15): # epochs + 1):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(mnist_trainset):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "\n",
    "        beta = kl_anneal_function('linear', step, 1, 10*len(mnist_trainset))\n",
    "            \n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        #if batch_idx == 500:\n",
    "        #    rewind_state_dict = copy.deepcopy(model.state_dict())\n",
    "        #    print('rewind state saved')\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(mnist_trainset.dataset),\n",
    "                100. * batch_idx / len(mnist_trainset),\n",
    "                loss.item() / len(data)))\n",
    "            plt_loss.append(loss.item() / len(data))  # For ploting loss\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(mnist_trainset.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 50.00%\n",
      "Linear(in_features=400, out_features=40, bias=True)\n",
      "Sparsity in Layer 1: 50.01%\n",
      "Linear(in_features=400, out_features=40, bias=True)\n",
      "Sparsity in Layer 2: 50.01%\n",
      "Linear(in_features=40, out_features=400, bias=True)\n",
      "Sparsity in Layer 3: 50.01%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 4: 50.00%\n",
      "Global Sparsity : 50.00%\n"
     ]
    }
   ],
   "source": [
    "from pruning import Pruning\n",
    "from pruning import PruningBack\n",
    "\n",
    "\n",
    "#local pruning\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        pr = Pruning(module)\n",
    "        prb = PruningBack(module)\n",
    "        pr.set_mask(0.7)\n",
    "        prb.set_mask(0.7)\n",
    "\n",
    "        module.register_forward_pre_hook(pr)\n",
    "        #module.register_backward_hook(prb)\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(mnist_trainset):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        break\n",
    "\n",
    "prstat = Pruning_tool()\n",
    "prstat.stats_pruning(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 131.466370\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 115.765579\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 119.548943\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 119.671738\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 123.205589\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 112.860954\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 113.997025\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 110.145615\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 108.668976\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 108.028481\n",
      "====> Epoch: 0 Average loss: 113.9240\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 99.202461\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 111.322105\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 102.749695\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 103.129349\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 96.888809\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 98.240273\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 96.409241\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 96.462883\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 95.361565\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 98.221970\n",
      "====> Epoch: 1 Average loss: 100.4699\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 100.064819\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 97.773346\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 101.686295\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 93.522491\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 103.111931\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 98.118126\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 98.708618\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 97.367249\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 100.448532\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 95.422783\n",
      "====> Epoch: 2 Average loss: 97.6759\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 0.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 0.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 0.01%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 0.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 0.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 0.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 0.01%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 0.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 0.00%\n",
      "Global Sparsity : 0.00%\n",
      "10.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 146.723404\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 132.929092\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 127.057343\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 127.999336\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 122.918274\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 119.905197\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 113.892441\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 108.434570\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 104.517654\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 111.176758\n",
      "====> Epoch: 0 Average loss: 120.6815\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 117.909187\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 113.700531\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 106.944031\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 104.187469\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 108.517082\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 104.376038\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 111.036255\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 101.877434\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 109.667274\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 103.572914\n",
      "====> Epoch: 1 Average loss: 106.9662\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 103.918594\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 106.105377\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 110.907883\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 103.447021\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 101.966568\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 99.845161\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 109.077118\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 96.608574\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 100.135071\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 100.598465\n",
      "====> Epoch: 2 Average loss: 104.5867\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 10.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 10.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 10.01%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 10.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 10.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 10.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 10.01%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 10.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 10.00%\n",
      "Global Sparsity : 10.00%\n",
      "20.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 161.206512\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 133.343079\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 131.152161\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 129.932358\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 120.948479\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 123.108849\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 114.132004\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 121.496567\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 119.045792\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 117.221054\n",
      "====> Epoch: 0 Average loss: 124.8941\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 116.523354\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 126.963844\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 109.750793\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 122.301453\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 117.538910\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 113.837288\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 115.236504\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 114.282288\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 110.995056\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 112.430138\n",
      "====> Epoch: 1 Average loss: 112.7437\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 107.290520\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 113.167557\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 107.263481\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 107.539391\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 113.221703\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 114.126022\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 111.600708\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 104.451706\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 116.002289\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 112.797684\n",
      "====> Epoch: 2 Average loss: 110.9372\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 20.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 20.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 20.00%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 20.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 20.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 20.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 20.00%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 20.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 20.00%\n",
      "Global Sparsity : 20.00%\n",
      "30.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 180.351654\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 141.534103\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 136.642517\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 125.122955\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 135.013046\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 123.978691\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 118.477066\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 119.069565\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 117.793900\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 113.783493\n",
      "====> Epoch: 0 Average loss: 126.7071\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 109.490776\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 118.503700\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 111.292221\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 116.148674\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 120.813057\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 102.793488\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 114.829880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 112.936470\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 113.707306\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 110.346634\n",
      "====> Epoch: 1 Average loss: 113.5518\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 106.689796\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 111.876030\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 115.199852\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 118.048347\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 110.380135\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 107.612885\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 111.636246\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 115.421379\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 109.992386\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 110.403625\n",
      "====> Epoch: 2 Average loss: 110.4400\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 30.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 30.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 30.00%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 30.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 30.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 30.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 30.00%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 30.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 30.00%\n",
      "Global Sparsity : 30.00%\n",
      "40.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 188.047913\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 139.576767\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 134.088425\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 125.739830\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 123.612968\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 119.898148\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 120.017548\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 123.477753\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 115.771584\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 114.768799\n",
      "====> Epoch: 0 Average loss: 125.2949\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 121.105331\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 110.467628\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 113.175583\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 112.030167\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 109.017212\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 105.477814\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 111.103943\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 111.856499\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 112.305115\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 105.403885\n",
      "====> Epoch: 1 Average loss: 112.1610\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 112.864952\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 110.248154\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 114.173126\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 117.005371\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 116.734726\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 105.689354\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 111.003296\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 104.711777\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 105.412895\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 103.570084\n",
      "====> Epoch: 2 Average loss: 109.6155\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 40.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 40.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 40.01%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 40.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 40.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 40.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 40.01%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 40.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 40.00%\n",
      "Global Sparsity : 40.00%\n",
      "50.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 193.188904\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 140.661942\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 131.871445\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 127.623428\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 126.549232\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 126.350723\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 121.751022\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 122.030975\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 110.745346\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 115.244698\n",
      "====> Epoch: 0 Average loss: 124.3130\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 114.068436\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 110.872696\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 110.271286\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 106.083931\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 109.832115\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 109.371078\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 109.368019\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 107.356056\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 107.768028\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 110.841034\n",
      "====> Epoch: 1 Average loss: 110.9885\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 110.085762\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 111.530243\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 104.557388\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 111.408447\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 107.514809\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 108.046745\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 105.687523\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 110.042099\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 113.195602\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 106.379791\n",
      "====> Epoch: 2 Average loss: 108.8573\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 50.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 50.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 50.01%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 50.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 50.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 50.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 50.01%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 50.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 50.00%\n",
      "Global Sparsity : 50.00%\n",
      "60.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 225.336258\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 141.253021\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 129.221573\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 124.338516\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 124.389450\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 117.267456\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 111.249908\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 108.823563\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 113.566238\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 109.402977\n",
      "====> Epoch: 0 Average loss: 124.5681\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 112.571594\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 114.974602\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 110.768852\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 105.769882\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 115.084541\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 106.383148\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 108.884659\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 108.536263\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 108.654205\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 108.083092\n",
      "====> Epoch: 1 Average loss: 110.3961\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 102.073563\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 116.667358\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 105.761421\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 105.476868\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 107.213394\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 108.005325\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 103.370155\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 110.399086\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 109.297623\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 109.750343\n",
      "====> Epoch: 2 Average loss: 108.4780\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 60.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 60.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 60.01%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 60.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 60.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 60.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 60.01%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 60.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 60.00%\n",
      "Global Sparsity : 60.00%\n",
      "70.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 271.058167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 146.366898\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 131.893097\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 131.008423\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 122.806183\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 119.148315\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 114.202148\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 113.233658\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 118.709518\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 113.151505\n",
      "====> Epoch: 0 Average loss: 127.7825\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 110.391785\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 111.289551\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 114.103569\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 103.167053\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 113.470070\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 119.172699\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 101.760689\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 107.062057\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 107.633301\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 113.697716\n",
      "====> Epoch: 1 Average loss: 110.5847\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 106.650131\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 109.025345\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 110.210564\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 110.349182\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 111.274734\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 109.674492\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 107.736298\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 104.678551\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 109.078026\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 102.635742\n",
      "====> Epoch: 2 Average loss: 108.4515\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 70.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 70.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 70.00%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 70.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 70.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 70.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 70.00%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 70.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 70.00%\n",
      "Global Sparsity : 70.00%\n",
      "80.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 337.302887\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 166.388885\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 153.344070\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 136.413620\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 133.516953\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 127.119232\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 126.570885\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 121.367935\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 121.476204\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 116.381020\n",
      "====> Epoch: 0 Average loss: 137.5300\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 119.849518\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 115.444801\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 111.393440\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 115.302483\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 113.678146\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 115.574692\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 104.742264\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 109.230209\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 105.832573\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 113.113815\n",
      "====> Epoch: 1 Average loss: 113.0305\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 106.854004\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 111.864471\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 102.264854\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 109.444656\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 110.517242\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 108.530197\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 111.747681\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 110.580032\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 108.330170\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 109.761398\n",
      "====> Epoch: 2 Average loss: 109.7717\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 80.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 80.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 80.00%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 80.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 80.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 80.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 80.00%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 80.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 80.00%\n",
      "Global Sparsity : 80.00%\n",
      "90.0 % of weights pruned\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 445.307404\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 198.439331\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 195.474701\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 181.833115\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 162.688339\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 159.975769\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 156.455627\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 151.064697\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 143.053055\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 141.160965\n",
      "====> Epoch: 0 Average loss: 174.5646\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 148.627960\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 139.739807\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 129.967163\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 133.462067\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 137.940796\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 137.225876\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 148.604660\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 133.390335\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 125.405563\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 127.330627\n",
      "====> Epoch: 1 Average loss: 134.5702\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 127.444641\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 128.735046\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 122.581429\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 123.918709\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 115.397644\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 126.047722\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 118.078476\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 117.103241\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 130.411682\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 127.718124\n",
      "====> Epoch: 2 Average loss: 124.1797\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 90.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 90.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 90.00%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 90.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 90.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 90.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 90.00%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 90.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 90.00%\n",
      "Global Sparsity : 90.00%\n",
      "Model :\n",
      "Linear(in_features=784, out_features=400, bias=True)\n",
      "Sparsity in Layer 0: 90.00%\n",
      "Linear(in_features=400, out_features=200, bias=True)\n",
      "Sparsity in Layer 1: 90.00%\n",
      "Linear(in_features=200, out_features=100, bias=True)\n",
      "Sparsity in Layer 2: 90.00%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 3: 90.03%\n",
      "Linear(in_features=100, out_features=32, bias=True)\n",
      "Sparsity in Layer 4: 90.03%\n",
      "Linear(in_features=32, out_features=100, bias=True)\n",
      "Sparsity in Layer 5: 90.03%\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "Sparsity in Layer 6: 90.00%\n",
      "Linear(in_features=200, out_features=400, bias=True)\n",
      "Sparsity in Layer 7: 90.00%\n",
      "Linear(in_features=400, out_features=784, bias=True)\n",
      "Sparsity in Layer 8: 90.00%\n",
      "Global Sparsity : 90.00%\n"
     ]
    }
   ],
   "source": [
    "from pruning import Pruning\n",
    "from pruning import PruningBack\n",
    "plt_loss = []\n",
    "prstat = Pruning_tool()\n",
    "\n",
    "pruning_amount = np.arange(10)/10\n",
    "\n",
    "for pa in pruning_amount:\n",
    "    print(pa*100,'% of weights pruned')\n",
    "    #local pruning\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            pr = Pruning(module)\n",
    "            prb = PruningBack(module)\n",
    "            pr.set_mask(pa)\n",
    "            prb.set_mask(pa)\n",
    "\n",
    "            module.register_forward_pre_hook(pr)\n",
    "            #module.register_backward_hook(prb)\n",
    "\n",
    "    for name, param in model.named_parameters(): \n",
    "        param.data = rewind_state_dict[name].clone()\n",
    "    for epoch in range(0, 3):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(mnist_trainset):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "\n",
    "            beta = kl_anneal_function('linear', step, 1, 10*len(mnist_trainset))\n",
    "            #print(beta)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(mnist_trainset.dataset),\n",
    "                    100. * batch_idx / len(mnist_trainset),\n",
    "                    loss.item() / len(data)))\n",
    "                plt_loss.append(loss.item() / len(data))  # For ploting loss\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(mnist_trainset.dataset)))\n",
    "    _, _, _ = model(data)\n",
    "    prstat.stats_pruning(model)\n",
    "\n",
    "prstat.stats_pruning(model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
